---
title: "Least Squares Approximation"
output: html_notebook
---



```{r}
library("MASS")
data = read.table("data.txt")
data = as.matrix(data)
```

# Least Square Approx
```{r}
x = as.matrix(data[,1])
y = as.matrix(data[,2])
phi = matrix(c(x^0,x^1), nrow=dim(data)[1], ncol=dim(data)[2])
w = ginv(phi)%*%y
plot(x,y)
<<<<<<< HEAD
y_approx = phi%*%w
lines(x,y_approx,col="green")
```

# Square Test
```{r}
y_mean = mean(y)
x_mean = mean(x)

SStot = sum((y-y_mean)**2)
SSres = sum((y-y_approx)**2)
r = 1 - (SSres/SStot)
```

# Cross-validation
```{r}
fold = 8
dimension_row = dim(data)[1]
n = dim(data)[1]/fold # divide into groups

# random distribution of numbers
tmp = runif(dimension_row,1,dimension_row) 
I = order(tmp)

# init MSE matrix
MSE = matrix(0, fold,1)
group_start = 1

# cross validation!
for (i in 1:fold){
  group_end = i*n
  test_set_indices = I[group_start:group_end]
  training_set_indices = setdiff(I,test_set_indices)
  
  test_set = data[test_set_indices,]
  training_set = data[training_set_indices,]
  
  x_test = test_set[,1]
  y_test = test_set[,2]

  x_train = training_set[,1]
  y_train = training_set[,2]
  
  phi_train = matrix(c(x_train^0,x_train^1), nrow = dimension_row-n , ncol = 2) # Do the regression
  w = ginv(phi_train)%*%y_train

  # Now we have learned the weights from the training data.
  # We need to determine how well these learned weights work for
  # the unseen test set points.

  phi_test = matrix(c(x_test^0,x_test^1), nrow = n , ncol = 2) 
  y_test_predicted = phi_test%*%w # Find the estimates of y_test

  MSE[i] =   sum((y_test-y_test_predicted)^2)/length(y_test)
  group_start = group_end + 1
}
barplot(MSE, beside = TRUE, names.arg = 1:dim(MSE)[1])
=======
lines(x,phi%*%w,col="green")
```

# Exercise 2
```{r}
library("MASS")
data = read.table("data.txt")
data = as.matrix(data) # Read data.txt as matrix

nrows = as.matrix(dim(data)[1]) 

tmp = as.matrix(rnorm(nrows)) # Get 168(nrows) random numbers & assign it to tmp
Y = sort(tmp) 
I = order(tmp)

dimensions = dim(data)
test_set_indices = I[1:floor(dimensions*0.2)] # Put dimensions[1] to remove the warning
training_set_indices = setdiff(I,test_set_indices)

numberOfTestPoints = floor(nrows*0.2)
numberOfTrainingPoints = nrows - numberOfTestPoints

#test_set_indices= 1:numberOfTestPoints  # First 20% indices are for testing
#training_set_indices = (ceiling(numberOfTestPoints)+1):nrows # Remaining 80% indices are for training

test_set = data[test_set_indices,] # Take the training & test data
training_set=data[training_set_indices,]

x_test = test_set[,1]
y_test = test_set[,2]

x_train = training_set[,1]
y_train = training_set[,2]

phi_train = matrix(c(x_train^0,x_train^1), nrow = numberOfTrainingPoints , ncol = 2) # Do the regression
w = ginv(phi_train)%*%y_train

# Now we have learned the weights from the training data.
# We need to determine how well these learned weights work for
# the unseen test set points.

phi_test = matrix(c(x_test^0,x_test^1), nrow = numberOfTestPoints , ncol = 2) 
y_test_predicted = phi_test%*%w # Find the estimates of y_test

plot(x_test,y_test)
par(new=TRUE) # hold on;
lines(x_test,y_test_predicted,col="green")
par(new=FALSE) # hold off;

sum((y_test-y_test_predicted)^2)/length(y_test)

sum((y_train - phi_train%*%w)^2)/length(y_train)

>>>>>>> kaslou
```