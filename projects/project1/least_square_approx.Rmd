---
title: "Least Squares Approximation"
output: html_notebook
---



```{r}
library("MASS")
data = read.table("data.txt")
data = as.matrix(data)
```

# Exercise 1
# Least Square Approximation
```{r}
x = as.matrix(data[,1])
y = as.matrix(data[,2])
phi = matrix(c(x^0,x^1), nrow=dim(data)[1], ncol=dim(data)[2])
w = ginv(phi)%*%y
plot(x,y)

y_approx = phi%*%w
lines(x,y_approx,col="green")
```

# Square Test
```{r}
y_mean = mean(y)
x_mean = mean(x)

SStot = sum((y-y_mean)**2)
SSres = sum((y-y_approx)**2)
r = 1 - (SSres/SStot)
```

# Exercise 2
# Generalization and Visualization
```{r}
library("MASS")
data = read.table("data.txt")
data = as.matrix(data) # Read data.txt as matrix

nrows = as.matrix(dim(data)[1]) 

tmp = as.matrix(rnorm(nrows)) # Get 168(nrows) random numbers & assign it to tmp
Y = sort(tmp) 
I = order(tmp)

dimensions = dim(data)
test_set_indices = I[1:floor(dimensions*0.2)] # Put dimensions[1] to remove the warning
training_set_indices = setdiff(I,test_set_indices)

numberOfTestPoints = floor(nrows*0.2)
numberOfTrainingPoints = nrows - numberOfTestPoints

#test_set_indices= 1:numberOfTestPoints  # First 20% indices are for testing
#training_set_indices = (ceiling(numberOfTestPoints)+1):nrows # Remaining 80% indices are for training

test_set = data[test_set_indices,] # Take the training & test data
training_set=data[training_set_indices,]

x_test = test_set[,1]
y_test = test_set[,2]

x_train = training_set[,1]
y_train = training_set[,2]

phi_train = matrix(c(x_train^0,x_train^1), nrow = numberOfTrainingPoints , ncol = 2) # Do the regression
w = ginv(phi_train)%*%y_train

# Now we have learned the weights from the training data.
# We need to determine how well these learned weights work for
# the unseen test set points.

phi_test = matrix(c(x_test^0,x_test^1), nrow = numberOfTestPoints , ncol = 2) 
y_test_predicted = phi_test%*%w # Find the estimates of y_test

plot(x_test,y_test)
par(new=TRUE) # hold on;
lines(x_test,y_test_predicted,col="green")
par(new=FALSE) # hold off;

sum((y_test-y_test_predicted)^2)/length(y_test)

sum((y_train - phi_train%*%w)^2)/length(y_train)
```
# Exercise 3
# Cross-validation
```{r}
fold = 8
dimension_row = dim(data)[1]
n = dim(data)[1]/fold # divide into groups

# random distribution of numbers
tmp = runif(dimension_row,1,dimension_row) 
I = order(tmp)

# init MSE matrix
MSE = matrix(0, fold,1)
group_start = 1

# cross validation!
for (i in 1:fold){
  group_end = i*n
  test_set_indices = I[group_start:group_end]
  training_set_indices = setdiff(I,test_set_indices)
  
  test_set = data[test_set_indices,]
  training_set = data[training_set_indices,]
  
  x_test = test_set[,1]
  y_test = test_set[,2]

  x_train = training_set[,1]
  y_train = training_set[,2]
  
  phi_train = matrix(c(x_train^0,x_train^1), nrow = dimension_row-n , ncol = 2) # Do the regression
  w = ginv(phi_train)%*%y_train

  # Now we have learned the weights from the training data.
  # We need to determine how well these learned weights work for
  # the unseen test set points.

  phi_test = matrix(c(x_test^0,x_test^1), nrow = n , ncol = 2) 
  y_test_predicted = phi_test%*%w # Find the estimates of y_test

  MSE[i] =   sum((y_test-y_test_predicted)^2)/length(y_test)
  group_start = group_end + 1
}
barplot(MSE, beside = TRUE, names.arg = 1:dim(MSE)[1])

lines(x,phi%*%w,col="green")
```
# Exercise 4:
# Recursive Least-Squares
```{r}
## script lin_rls.R
##
rm(list=ls())
par(ask=TRUE)
n<-1;

rls<-function(x,y,t,P,mu=1){

P.new <-(P-(P%*%x%*%x%*%P)/as.numeric(1+x%*%P%*%x))/mu
ga <- P.new%*%x
epsi <- y-x%*%t
t.new<-t+ga*as.numeric(epsi)
list(t.new,P.new)
}

X<-seq(-pi,pi,by=.02)
N<-length(X)

y<-sin(X)+0.1*rnorm(N)
t<-numeric(2)
P<-500*diag(n+1)
mu<-0.9
for (i in 1:N){
    rls.step<-rls(c(1, X[i]),y[i],t,P,mu)
    t<-rls.step[[1]]
    P<-rls.step[[2]]
    plot(X[1:i],y[1:i], xlim=c(-4,4), ylim=c(-2,2), main=paste("Forgetting factor mu<-",mu))

    lines(X[1:i],cbind(array(1,c(i,1)), X[1:i])%*%t, col="red",) 
}

```