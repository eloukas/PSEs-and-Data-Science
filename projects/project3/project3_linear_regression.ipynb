{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Simple Linear Regression Models\n",
    "”There are three kinds of lies: lies, damned lies and statistics.”\n",
    "— Mark Twain\n",
    "\n",
    "## Simple linear regression models\n",
    "    Response Variable: Estimated variable\n",
    "    Predictor Variables: Variables used to predict the response\n",
    "    Also called predictors or factors\n",
    "    Regression Model: Predict a response for a given set of predictor variables\n",
    "    Linear Regression Models: Response is a linear function of predictors\n",
    "    Simple Linear Regression Models: Only one predictor\n",
    "    \n",
    "## Outline\n",
    "* Definition of a Good Model\n",
    "* **Estimation of Model parameters**\n",
    "* Allocation of Variation\n",
    "* Standard deviation of Errors\n",
    "* Confidence Intervals for Regression Parameters\n",
    "* Confidence Intervals for Predictions\n",
    "* Visual Tests for verifying Regression Assumption\n",
    "\n",
    "## 2 Definition of a good regression models?\n",
    "![](./images/1.png)\n",
    "\n",
    "Regression models attempt to fit lines (or curves) to the observation points\n",
    "(data) that minimize the vertical distance between the observation point\n",
    "and the model line (or curve). The length of this distance is called residual,\n",
    "modeling error, or simply error. The negative and positive errors should cancel\n",
    "out => Zero overall error\n",
    "It is obvious that many lines will satisfy this criterion.\n",
    "\n",
    "## 2.1 Linear Regression Model:\n",
    "Given n observation pairs ${(x_1, y_1 ),..., (x_n, y_n)}$, the estimated response for\n",
    "the i-th observation is\n",
    "$\\hat y_i = b_0 + b_1*x_i$ where the regression parameters $b_0$ and $b_1$ are chosen that\n",
    "minimizes the sum of squares of the errors at the given data (observations).\n",
    "\n",
    "Formally, the model has the form:\n",
    "$\\hat y = b_0 + b_1*x$ where, $\\hat y$ is the predicted response when the predictor variable is $x$.\n",
    "The error is:\n",
    "\n",
    "$ e_i = y_i - \\hat y_i and \\sum_{i=1}^n (e_i)^2 = \\sum_{i=1}^n (y_i - b_0 - b_1*x_i)^2 $\n",
    "\n",
    "The best linear model minimizes the sum of squared errors (SSE(, subject to the constraint that ther overall mean error is zero:\n",
    "\n",
    "$\\sum_{i=1}^n (e_i) = \\sum_{i=1}^n (y_i - b_0 - b_1*x_i) = 0$\n",
    "\n",
    "## Linear Regressional Model - the statistical view\n",
    "Regression analysis is the art and science of fitting straight lines to patterns\n",
    "of data. In a linear regression model, the variable of interest (the so-called\n",
    "“dependent” variable) is predicted from\n",
    "other variable(s) (the so-called “independent” variable(s)) using a linear\n",
    "equation. \n",
    "\n",
    "If $Y$ denotes the dependent variable, and $X_1, X_2 ,...,X_{\\kappa}$ , are the\n",
    "independent variables, then the assumption is that the value of $Y_i$ in the population is determined by the linear equation $Y_i = \\beta_0 + \\beta_1*X_{i1} + \\beta_2*X_{i2} + ... + \\beta_{\\kappa}*X_{i\\kappa} + \\epsilon_i$ where the betas are constants and\n",
    "\n",
    "the epsilons are independent and\n",
    "identically distributed (i.i.d.) normal random variables with mean zero (the\n",
    "“noise” in the system). \n",
    "\n",
    "$\\beta_0$ is the so-called intercept of the model—the expected\n",
    "value of $Y$ when all the $X$'s are zero and i is the coefficient (multiplier) of the variable $X_i$. **The betas to-\n",
    "gether with the mean and standard deviation of the epsilons are the\n",
    "parameters of the model.**\n",
    "\n",
    "The corresponding equation for predicting Y i from the corresponding values\n",
    "of the X’s is therefore where the b’s are estimates of the betas obtained by\n",
    "least-squares, i.e., minimizing the square\n",
    "prediction error within the sample. \t__*Multiple regression allows more than\n",
    "one x variables.*__\n",
    "\n",
    "**Assumptions**\n",
    "The error terms $\\epsilon_i$ are mutually independent and identically distributed,\n",
    "with mean = 0 and constant variances $E[\\epsilon_i] = 0 ~ V[\\epsilon_i ] = \\sigma^2$\n",
    "This is so, because the observations $Y_1 , Y_2 , ..., Y_\\kappa$ are a random sample,\n",
    "they are mutually independent and hence the error terms are also mutually\n",
    "independent.\n",
    "\n",
    "The distribution of the error term is independent of the joint distribution of\n",
    "$X_1, X_2 ,...,X_\\kappa$. The unknown parameters $\\beta_0 , \\beta_1 , \\beta_2 , ..., \\beta_\\kappa$ are constants.\n",
    "\n",
    "## 2.2.1 Summary of multiple linear regression model\n",
    "**Independent variables:** $Χ_1, Χ_2, ...., Χ_n$\n",
    "\n",
    "**Data:** ${(y_1 , x_{11} , x_{21} , ..., x_{k1} ), .., (y_n , , x_{n1} , x_{2n} , ..., x_{kn} )}$\n",
    "\n",
    "**Population Model:** $Y_i = \\beta_0 + \\beta_1*X_{i1} + \\beta_2*X_{i2} + ... + \\beta_{\\kappa}*X_{i\\kappa} + \\epsilon_i$ where $\\epsilon_i$ are i.i.d. random variables following the normal disribution $N(0, \\sigma)$\n",
    "\n",
    "**Regression coefficients:** $b_0,b_1,....,b_k$ are estimates of $\\beta_0,\\beta_1,....,\\beta_k$\n",
    "\n",
    "**Regression Estimates of $Y_i: \\hat y_i = b_0 + b_1*x_{i1} + b_2*x_{i2} + ... + b_k*x_{i\\kappa}$**\n",
    "\n",
    "**Goal:** Choose $b_0,b_1...,b_k$ to minimize the residual sum of squares $\\sum_{i=1}^n e^2 = \\sum_{i=1}^n (y_i -\\hat y_i)^2$\n",
    "\n",
    "## 2.2.2 Summary of single variable linear regression model\n",
    "Assuming that the data is a subset of a population then the linear regression\n",
    "model can be described as follows:\n",
    "\n",
    "**Data**: ${(x_1 , y_1 ), . . . , (x_n , y_n )}$\n",
    "\n",
    "**Model of the population**: $y_i = \\beta_0 + \\beta_1*x_i i + \\epsilon_i$\n",
    "\n",
    "where $\\epsilon_1 ,\\epsilon_2 , ..., \\epsilon_n$ are independent and identically distributed (i.i.d.) ran-\n",
    "dom variables, with normal distribution N(0,\\sigma)\n",
    "This is the true relation between $y$ and $x$ that depends on the estimation of\n",
    "the unknows $\\beta_0$ and $\\beta_1$ based on a sample (data) of the population.\n",
    "\n",
    "Comments:\n",
    "$E(y_i | x_i ) = \\beta_0 + \\beta_1*x_i$\n",
    "$SD(y_i|x_i) = \\sigma$\n",
    "Relationship is linear - described by a \"line\"\n",
    "$\\beta_0$ = \"baseline\" value of (i.e., value of $y$ if $x$ is 0)\n",
    "$\\beta_1$ = \"slope\" of line (average change in y per unit change in $x$)\n",
    "\n",
    "**Prediction regression model:**\n",
    "$\\hat y_i = b_0 + b_1*x_i$\n",
    "where the b’s are estimates of the betas obtained by least-squares, i.e., min-\n",
    "imizing the square prediction error within the sample.\n",
    "\n",
    "![](./images/2.png)\n",
    "\n",
    "## Outline\n",
    "* Definition of a Good Model\n",
    "* **Estimation of Model parameters**\n",
    "* Allocation of Variation\n",
    "* Standard deviation of Errors\n",
    "* Confidence Intervals for Regression Parameters\n",
    "* Confidence Intervals for Predictions\n",
    "* Visual Tests for verifying Regression Assumption\n",
    "\n",
    "## 3 Estimation of model parameters \n",
    "![](./images/3.png)\n",
    "\n",
    "## Example 1\n",
    "The number of disk I/O's and processor time of seven programs were measured as\n",
    "![](./images/4.png)\n",
    "![](./images/5.png)\n",
    "**Error Computation**\n",
    "![](./images/6.png)\n",
    "\n",
    "## Outline\n",
    "* Definition of a Good Model\n",
    "* Estimation of Model parameters\n",
    "* **Allocation of Variation**\n",
    "* Standard deviation of Errors\n",
    "* Confidence Intervals for Regression Parameters\n",
    "* Confidence Intervals for Predictions\n",
    "* Visual Tests for verifying Regression Assumption\n",
    "\n",
    "## 4 Allocation of variation\n",
    "**Error variance from the sample mean = Variance of the response from\n",
    "the mean value of the observation**\n",
    "Error = $\\epsilon_i$ = Observed Response - Predicted Response from the mean value\n",
    "= $y_i - \\bar y$\n",
    "\n",
    "Variance of Errors from the sample mean = $\\frac{1}{n} \\sum_{i=1}^n (e_i)^2 = \\frac{1}{n} (y_i - \\bar y)^2 =$\n",
    "variance of y\n",
    "\n",
    "Note that the standard error of the model is not the square root of the\n",
    "average value of the squared\n",
    "errors within the historical sample of data. Rather, the sum of squared errors\n",
    "is divided by $n - 1$\n",
    "rather than n under the square root sign because this adjusts for the fact\n",
    "that a ”degree of freedom for error\"\n",
    "has been used up by estimating one model parameter (namely the mean)\n",
    "from the sample of $n$ data points.\n",
    "\n",
    "The sum of squared errors from the sample mean $SST = \\sum_{i=1}^n (y_i - \\bar y)^2$ is\n",
    "called total sum of squares.\n",
    "\n",
    "It is a measure of y’s variability and is called variation of y. SST can be\n",
    "computed as follows:\n",
    "$SST = \\sum_{i=1}^n (y_i - \\bar y)^2 = (\\sum_{i=1}^n (y_i)^2 - n \\bar y^2 = SSY - SS0$\n",
    "Where, $SSY$ is the sum of squares of $y$ and $SS0$ is the sum of squares of $\\bar y$\n",
    "and is equal to $n\\bar y^2$\n",
    "\n",
    "The difference between SST ans SSE is the sum of squares explained by the\n",
    "regression.\n",
    "\n",
    "It is called SSR: SSR = SST - SSE or SST = SSR + SSE\n",
    "\n",
    "The fraction of the variation that is explained determines the goodness of\n",
    "the regression and it is called the coeffiecient of tetermination, $R^2 = \\frac{SSR}{SST} = \\frac{SST - SSE}{SST} = 1 - \\frac{SSE}{SST}$\n",
    "\n",
    "The higher the value of R^2 the better the regressiona $R^2 = 1 ->$ perfect fit\n",
    "$R^2 = 0 ->$ No fit\n",
    "![](./images/7.png)\n",
    "\n",
    "## Example 3\n",
    "For the disk I/O-CPU time data: $SSE = 5.87$ and $SST = 205.71$ and $SSR =\n",
    "199.84$ and $R^2 = 0.9715$\n",
    "The linear regression explains $97\\%$ of CPU time’s variation.\n",
    "\n",
    "## Outline\n",
    "* Definition of a Good Model\n",
    "* Estimation of Model parameters\n",
    "* Allocation of Variation\n",
    "* **Standard deviation of Errors**\n",
    "* Confidence Intervals for Regression Parameters\n",
    "* Confidence Intervals for Predictions\n",
    "* Visual Tests for verifying Regression Assumption\n",
    "\n",
    "## Standard deviation of errors\n",
    "Since errors are obtained after calculating two regression parameters from the\n",
    "data, errors have $n - 2$ degrees of freedom\n",
    "$SSE/(n-2)$ is called mean squared errors or (MSE)\n",
    "$S_{e}^2 = \\frac{SSE}{n-2}$\n",
    "Standard deviation of errors = square root of MSE\n",
    "\n",
    "Note:\n",
    "SSY has ndegrees of freedom since it is obtained from n independent\n",
    "observations without estimating any parameters\n",
    "\n",
    "SS0 has just one degree of freedom since it can be computed simply\n",
    "from y\n",
    "\n",
    "SST has n 1 degrees of freedom, since one parameter must be\n",
    "calculated from the data before SST can be computed\n",
    "\n",
    "SSR, which is the di↵erence between SST and SSE, has the remain-\n",
    "ing one degree of freedom.\n",
    "\n",
    "Overall,\n",
    "\n",
    "$SST = SSY - SS0 = SSR + SSE$\n",
    "$n - 1 = n - 1 = 1 + (n-2)$\n",
    "Notice that the degrees of freedom add just the way the sums of squares do.\n",
    "\n",
    "## Example\n",
    "For the disk I/O-CPU data we have\n",
    "\n",
    "$SS: SST(205.71) = SSy(828) - SS0 (622.29) = SSR (199.84) + SSE(5.87)$\n",
    "\n",
    "$DF: SST(6) = SSy(7) - SS0 (1) = SSR (1) + SSE(5)$\n",
    "\n",
    "The mean squared error is:\n",
    "\n",
    "MSE = /DF for Errors = 5.87/5 = $1.174$\n",
    "\n",
    "The standard deviation of errors is:\n",
    "\n",
    "$s_e = sqrt(MSE) = sqrt(1.174) = 1.0835$\n",
    "\n",
    "\n",
    "## Outline\n",
    "* Definition of a Good Model\n",
    "* Estimation of Model parameters\n",
    "* Allocation of Variation\n",
    "* Standard deviation of Errors\n",
    "* **Confidence Intervals (CI) for Regression Parameters**\n",
    "* Confidence Intervals for Predictions\n",
    "* Visual Tests for verifying Regression Assumption\n",
    "\n",
    "## 6 Regression Statistics\n",
    "![](./images/8.png) \n",
    "\n",
    "## 7 CIs for regression parameters\n",
    "1. Regression coefficients $b_0$ and $b_1$ are estimates from a single random sample\n",
    "of size $n\\ge1$.\n",
    "2. Using another sample, the estimates may be different.\n",
    "\n",
    "**Ιf $\\beta_0$ and $\\beta_1$ are true parameters of the population (i.e., $y = b_0 +\n",
    "b_1x$), then the computed coefficients $b_0$ and $b_1$ are estimates of $\\beta_0$ and\n",
    "$\\beta_1$ , respectively.**\n",
    "\n",
    "Sample standard deviation of $b_0$ and $b_1$\n",
    "![](./images/9.png) \n",
    "\n",
    "The $100(1-a)\\%$ confidence intervals for $b_0$ and $b_1$ can be computed using\n",
    "$t[1-a/2; n-2] -$ the $1 - a/2$ quantile of a $t$ variate with $n-2$ degrees of\n",
    "freedom.\n",
    "The confidence intervals are:\n",
    "\n",
    "$b_0 \\pm ts*b_0$\n",
    "\n",
    "$b_1 \\pm ts*b_1$\n",
    "\n",
    "If a confidence interval includes zero, then the regression parameter cannot\n",
    "be considered different from zero at the\n",
    "100(1-a)% confidence level\n",
    "\n",
    "## Example\n",
    "![](./images/10.png) \n",
    "\n",
    "## Case study: remote procedure call\n",
    "![](./images/11.png) \n",
    "\n",
    "## Outline\n",
    "* Definition of a Good Model\n",
    "* Estimation of Model parameters\n",
    "* Allocation of Variation\n",
    "* Standard deviation of Errors\n",
    "* **Confidence Intervals (CI) for Regression Parameters**\n",
    "* Confidence Intervals for Predictions\n",
    "* Visual Tests for verifying Regression Assumption\n",
    "\n",
    "## 8 CI for predications \n",
    "$$ \\hat y_p = b_0 + b_1x_p $$\n",
    "![](./images/12.png) \n",
    "![](./images/13.png) \n",
    "\n",
    "\n",
    "## Outline\n",
    "* Definition of a Good Model\n",
    "* Estimation of Model parameters\n",
    "* Allocation of Variation\n",
    "* Standard deviation of Errors\n",
    "* Confidence Intervals (CI) for Regression Parameters\n",
    "* Confidence Intervals for Predictions\n",
    "* Visual Tests for verifying Regression Assumption\n",
    "\n",
    "## 9 Visual test for regress assumptions\n",
    "Regression assumptions:\n",
    "The true relationship between the response variable y and the predictor\n",
    "variable x is linear.\n",
    "The predictor variable x is non-stochastic and it is measured without any\n",
    "error.\n",
    "The model errors are statistically independent.\n",
    "The errors are normally distributed with zero mean and a constant standard\n",
    "deviation.\n",
    "![](./images/14.png) \n",
    "![](./images/15.png) \n",
    "![](./images/16.png) \n",
    "![](./images/17.png) \n",
    "![](./images/18.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
