{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Simple Linear Regression Models\n",
    "”There are three kinds of lies: lies, damned lies and statistics.”\n",
    "— Mark Twain\n",
    "\n",
    "## Simple linear regression models\n",
    "    Response Variable: Estimated variable\n",
    "    Predictor Variables: Variables used to predict the response\n",
    "    Also called predictors or factors\n",
    "    Regression Model: Predict a response for a given set of predictor variables\n",
    "    Linear Regression Models: Response is a linear function of predictors\n",
    "    Simple Linear Regression Models: Only one predictor\n",
    "    \n",
    "## Outline\n",
    "* Definition of a Good Model\n",
    "* **Estimation of Model parameters**\n",
    "* Allocation of Variation\n",
    "* Standard deviation of Errors\n",
    "* Confidence Intervals for Regression Parameters\n",
    "* Confidence Intervals for Predictions\n",
    "* Visual Tests for verifying Regression Assumption\n",
    "\n",
    "## 2 Definition of a good regression models?\n",
    "![](./images/1.png)\n",
    "\n",
    "Regression models attempt to fit lines (or curves) to the observation points\n",
    "(data) that minimize the vertical distance between the observation point\n",
    "and the model line (or curve). The length of this distance is called residual,\n",
    "modeling error, or simply error. The negative and positive errors should cancel\n",
    "out => Zero overall error\n",
    "It is obvious that many lines will satisfy this criterion.\n",
    "\n",
    "## 2.1 Linear Regression Model:\n",
    "Given n observation pairs {(x_1, y_1 ),..., (x_n, y_n)}, the estimated response for\n",
    "the i-th observation is\n",
    "\\hat y_i = b_0 + b_1*x_i where the regression parameters b_0 and b_1 are chosen that\n",
    "minimizes the sum of squares of the errors at the given data (observations).\n",
    "\n",
    "Formally, the model has the form:\n",
    "\\hat y = b_0 + b_1*x where, \\hat y is the predicted response when the predictor variable is x.\n",
    "The error is:\n",
    "\n",
    "e_i = y_i - \\hat y_i and \\sum_{i=1}^n (e_i)^2 = \\sum_{i=1}^n (y_i - b_0 - b_1*x_i)^2 \n",
    "\n",
    "The best linear model minimizes the sum of squared errors (SSE(, subject to the constraint that ther overall mean error is zero:\n",
    "\n",
    "\\sum_{i=1}^n (e_i) = \\sum_{i=1}^n (y_i - b_0 - b_1*x_i) = 0\n",
    "\n",
    "## Linear Regressional Model - the statistical view\n",
    "Regression analysis is the art and science of fitting straight lines to patterns\n",
    "of data. In a linear regression model, the variable of interest (the so-called\n",
    "“dependent” variable) is predicted from\n",
    "other variable(s) (the so-called “independent” variable(s)) using a linear\n",
    "equation. \n",
    "\n",
    "If Y denotes the dependent variable, and X 1 , X 2 ,...,X  , are the\n",
    "independent variables, then the assumption is that the value of Y_i in the population is determined by the linear equation\n",
    "Y_i = β_0 + β_1*X_i1 + β_2*X_i2 + ... + β_κ*X_iκ + ε_i where the betas are constants and\n",
    "the epsilons are independent and\n",
    "identically distributed (i.i.d.) normal random variables with mean zero (the\n",
    "“noise” in the system). \n",
    "\n",
    "β_0 is the so-called intercept of the model—the expected\n",
    "value of Y when all the X’s are zero and i is the coefficient (multiplier) of the variable X_i. **The betas to-\n",
    "gether with the mean and standard deviation of the epsilons are the\n",
    "parameters of the model.**\n",
    "\n",
    "The corresponding equation for predicting Y i from the corresponding values\n",
    "of the X’s is therefore where the b’s are estimates of the betas obtained by\n",
    "least-squares, i.e., minimizing the square\n",
    "prediction error within the sample. \t__*Multiple regression allows more than\n",
    "one x variables.*__\n",
    "\n",
    "**Assumptions**\n",
    "The error terms ε_i are mutually independent and identically distributed,\n",
    "with mean = 0 and constant variances E[ε_i] = 0 V [ε_i ] = σ^2\n",
    "This is so, because the observations Y_1 , Y_2 , ..., Y_κ are a random sample,\n",
    "they are mutually independent and hence the error terms are also mutually\n",
    "independent.\n",
    "\n",
    "The distribution of the error term is independent of the joint distribution of\n",
    "X_1 , X_2 ,...,X_κ . The unknown parameters β_0 , β_1 , β_2 , ..., β_κ are constants.\n",
    "\n",
    "## 2.2.1 Summary of multiple linear regression model\n",
    "**Independent variables:** Χ_1, Χ_2, ...., Χ_n\n",
    "**Data:** {(y_1 , x_11 , x_21 , ..., x_k1 ), .., (y_n , , x_n1 , x_2n , ..., x_kn )}\n",
    "**Population Model:** Y_i = β_0 + β_1*X_i1 + β_2*X_i2 + ... + β_κ*X_iκ + ε_i where ε_i are i.i.d. random variables following the normal disribution N (0, σ)\n",
    "**Regression coefficients:** b_0,b_1,....,b_k are estimates of β_0,β_1,....,β_k\n",
    "**Regression Estimates of Y_i: \\hat y_i = b_0 + b_1*x_i1 + b_2*x_i2 + ... + b_k*x_iκ**\n",
    "**Goal:** Choose b_0,b_1...,b_k to minimize the residual sum of squares \\sum_{i=1}^n e^2 = \\sum_{i=1}^n (y_i -\\hat y_i)^2\n",
    "\n",
    "## 2.2.2 Summary of single variable linear regression model\n",
    "Assuming that the data is a subset of a population then the linear regression\n",
    "model can be described as follows:\n",
    "**Data**: {(x_1 , y_1 ), . . . , (x_n , y_n )}\n",
    "\n",
    "**Model of the population**:y_i = β_0 + β_1*x_i i + ε_i\n",
    "\n",
    "where ε_1 ,ε_2 , ..., ε_n are independent and identically distributed (i.i.d.) ran-\n",
    "dom variables, with normal distribution N(0,σ)\n",
    "This is the true relation between y and x that depends on the estimation of\n",
    "the unknows β_0 and β_1 based on a sample (data) of the population.\n",
    "\n",
    "Comments:\n",
    "E(y_i | x_i ) = β_0 + β_1*x_i\n",
    "SD(y_i|x_i) = σ\n",
    "Relationship is linear - described by a \"line\"\n",
    "β_0 = \"baseline\" value of (i.e., value of y if x is 0)\n",
    "β_1 = \"slope\" of line (average change in y per unit change in x)\n",
    "\n",
    "**Prediction regression model:**\n",
    "\\hat y_i = b_0 + b_1*x_i\n",
    "where the b’s are estimates of the betas obtained by least-squares, i.e., min-\n",
    "imizing the square prediction error within the sample.\n",
    "\n",
    "![](./images/2.png)\n",
    "\n",
    "## Outline\n",
    "* Definition of a Good Model\n",
    "* **Estimation of Model parameters**\n",
    "* Allocation of Variation\n",
    "* Standard deviation of Errors\n",
    "* Confidence Intervals for Regression Parameters\n",
    "* Confidence Intervals for Predictions\n",
    "* Visual Tests for verifying Regression Assumption\n",
    "\n",
    "## 3 Estimation of model parameters \n",
    "![](./images/3.png)\n",
    "\n",
    "## Example 1\n",
    "The number of disk I/O's and processor time of seven programs were measured as\n",
    "![](./images/4.png)\n",
    "![](./images/5.png)\n",
    "**Error Computation**\n",
    "![](./images/6.png)\n",
    "\n",
    "## Outline\n",
    "* Definition of a Good Model\n",
    "* Estimation of Model parameters\n",
    "* **Allocation of Variation**\n",
    "* Standard deviation of Errors\n",
    "* Confidence Intervals for Regression Parameters\n",
    "* Confidence Intervals for Predictions\n",
    "* Visual Tests for verifying Regression Assumption\n",
    "\n",
    "## 4 Allocation of variation\n",
    "**Error variance from the sample mean = Variance of the response from\n",
    "the mean value of the observation**\n",
    "Error = ε_i = Observed Response - Predicted Response from the mean value\n",
    "= y_i - \\bar y\n",
    "\n",
    "Variance of Errors from the sample mean = \\frac{1}{n} \\sum_{i=1}^n (e_i)^2 = \\frac{1}{n} (y_i - \\bar y)^2 =\n",
    "variance of y\n",
    "\n",
    "Note that the standard error of the model is not the square root of the\n",
    "average value of the squared\n",
    "errors within the historical sample of data. Rather, the sum of squared errors\n",
    "is divided by n - 1\n",
    "rather than n under the square root sign because this adjusts for the fact\n",
    "that a ”degree of freedom for error ε\"\n",
    "has been used up by estimating one model parameter (namely the mean)\n",
    "from the sample of n data points.\n",
    "\n",
    "The sum of squared errors from the sample mean SST = \\sum_{i=1}^n (y_i - \\bar y)^2 is\n",
    "called total sum of squares.\n",
    "\n",
    "It is a measure of y’s variability and is called variation of y. SST can be\n",
    "computed as follows:\n",
    "SST = \\sum_{i=1}^n (y_i - \\bar y)^2 = (\\sum_{i=1}^n (y_i)^2 - n \\bar y^2 = SSY - SS0\n",
    "Where, SSY is the sum of squares of y and SS0 is the sum of squares of \\bar y\n",
    "and is equal to n\\bar y^2\n",
    "\n",
    "The difference between SST ans SSE is the sum of squares explained by the\n",
    "regression.\n",
    "\n",
    "It is called SSR: SSR = SST - SSE or SST = SSR + SSE\n",
    "\n",
    "The fraction of the variation that is explained determines the goodness of\n",
    "the regression and it is called the coeffiecient of tetermination, R^2 = SSR / SST = (SST - SSE) / SST = 1 - (SSE/SST)\n",
    "\n",
    "The higher the value of R^2 the better the regression R^2 = 1 -> perfect fit\n",
    "R^2 = 0 -> No fit\n",
    "![](./images/7.png)\n",
    "\n",
    "## Example 3\n",
    "For the disk I/O-CPU time data: SSE = 5.87 and SST = 205.71 and SSR =\n",
    "199.84 and R^2 = 0.9715\n",
    "The linear regression explains 97% of CPU time’s variation.\n",
    "\n",
    "## Outline\n",
    "* Definition of a Good Model\n",
    "* Estimation of Model parameters\n",
    "* Allocation of Variation\n",
    "* **Standard deviation of Errors**\n",
    "* Confidence Intervals for Regression Parameters\n",
    "* Confidence Intervals for Predictions\n",
    "* Visual Tests for verifying Regression Assumption\n",
    "\n",
    "## Standard deviation of errors\n",
    "Since errors are obtained after calculating two regression parameters from the\n",
    "data, errors have n 2 degrees of freedom\n",
    "SSE/(n-2) is called mean squared errors or (MSE)\n",
    "S_{e}^2 = SSE/(n-2)\n",
    "Standard deviation of errors = square root of MSE\n",
    "\n",
    "Note:\n",
    "SSY has ndegrees of freedom since it is obtained from n independent\n",
    "observations without estimating any parameters\n",
    "SS0 has just one degree of freedom since it can be computed simply\n",
    "from y\n",
    "SST has n 1 degrees of freedom, since one parameter must be\n",
    "calculated from the data before SST can be computed\n",
    "SSR, which is the di↵erence between SST and SSE, has the remain-\n",
    "ing one degree of freedom.\n",
    "Overall,\n",
    "SST = SSY SS0 = SSR + SSE\n",
    "n - 1 = n - 1 = 1 + (n-2)\n",
    "Notice that the degrees of freedom add just the way the sums of squares do.\n",
    "\n",
    "## Example\n",
    "For the disk I/O-CPU data we have\n",
    "or the disk I/O-CPU data we have\n",
    "SS: SST(205.71) = SSy(828) - SS0 (622.29) = SSR (199.84) + SSE(5.87)\n",
    "DF: SST(6) = SSy(7) - SS0 (1) = SSR (1) + SSE(5)\n",
    "The mean squared error is:\n",
    "MSE = SSE/DF for Errors = 5.87/5 = 1.174\n",
    "The standard deviation of errors is:\n",
    "s_e = sqrt(MSE) = sqrt(1.174) = 1.0835\n",
    "\n",
    "\n",
    "## Outline\n",
    "* Definition of a Good Model\n",
    "* Estimation of Model parameters\n",
    "* Allocation of Variation\n",
    "* Standard deviation of Errors\n",
    "* **Confidence Intervals (CI) for Regression Parameters**\n",
    "* Confidence Intervals for Predictions\n",
    "* Visual Tests for verifying Regression Assumption\n",
    "\n",
    "## 6 Regression Statistics\n",
    "![](./images/8.png) \n",
    "\n",
    "## 7 CIs for regression parameters\n",
    "1. Regression coefficients b 0 and b 1 are estimates from a single random sample\n",
    "of size n>=1.\n",
    "2. Using another sample, the estimates may be different.\n",
    "\n",
    "**Ιf β_0 and β_1 are true parameters of the population (i.e., y = b_0 +\n",
    "b_1x), then the computed coefficients b_0 and b_1 are estimates of b_0 and\n",
    "β_1 , respectively.**\n",
    "\n",
    "Sample standard deviation of b_0 and b_1\n",
    "![](./images/9.png) \n",
    "\n",
    "The 100(1-a)% confidence intervals for b 0 and b 1 can be computed using\n",
    "t[1-a/2; n-2] ---- the 1 - a/2 quantile of a t variate with n-2 degrees of\n",
    "freedom.\n",
    "The confidence intervals are:\n",
    "b_0 -+ ts*b_0\n",
    "b_1 -+ ts*b_1\n",
    "If a confidence interval includes zero, then the regression parameter cannot\n",
    "be considered different from zero at the\n",
    "100(1-a)% confidence level\n",
    "\n",
    "## Example\n",
    "![](./images/10.png) \n",
    "\n",
    "## Case study: remote procedure call\n",
    "![](./images/11.png) \n",
    "\n",
    "## Outline\n",
    "* Definition of a Good Model\n",
    "* Estimation of Model parameters\n",
    "* Allocation of Variation\n",
    "* Standard deviation of Errors\n",
    "* **Confidence Intervals (CI) for Regression Parameters**\n",
    "* Confidence Intervals for Predictions\n",
    "* Visual Tests for verifying Regression Assumption\n",
    "\n",
    "## 8 CI for predications \n",
    "$$ \\hat y_p = b_0 + b_1x_p $$\n",
    "![](./images/12.png) \n",
    "![](./images/13.png) \n",
    "\n",
    "\n",
    "## Outline\n",
    "* Definition of a Good Model\n",
    "* Estimation of Model parameters\n",
    "* Allocation of Variation\n",
    "* Standard deviation of Errors\n",
    "* Confidence Intervals (CI) for Regression Parameters\n",
    "* Confidence Intervals for Predictions\n",
    "* Visual Tests for verifying Regression Assumption\n",
    "\n",
    "## 9 Visual test for regress assumptions\n",
    "Regression assumptions:\n",
    "The true relationship between the response variable y and the predictor\n",
    "variable x is linear.\n",
    "The predictor variable x is non-stochastic and it is measured without any\n",
    "error.\n",
    "The model errors are statistically independent.\n",
    "The errors are normally distributed with zero mean and a constant standard\n",
    "deviation.\n",
    "![](./images/14.png) \n",
    "![](./images/15.png) \n",
    "![](./images/16.png) \n",
    "![](./images/17.png) \n",
    "![](./images/18.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
